---
title: "Generative AI Misinformation Detection (2024-2025) - Final Report"
author: "Mahathi Reddy, Maya Manal France, Nayan Menezes"
date: "Dec. 8, 2025"
output: html_document
---

# 1. Introduction

The launch of ChatGPT in November 2022 marked a major shift in how people interact with technology, particularly through the ability to generate detailed text on virtually any topic within seconds. Alongside other generative AI tools such as Claude and Gemini, these systems allow users to produce content—ranging from opinions on politics and sports to music and culture — with minimal independent research. Increasingly, this AI-generated material finds its way onto social media platforms, often without users verifying its accuracy or even recognizing that it was created by an artificial system. In response, platforms such as X have begun flagging posts that may contain AI-generated content in an effort to help users distinguish between human-authored information and automated output.

The dangers of unchecked AI-generated media became especially clear in November 2025, when a deepfake video circulated on social media that altered a 2018 speech by U.S. politician DeAndrea Salvador to make it appear as though she was addressing low-income communities in São Paulo, Brazil. The manipulated video was later revealed to have been used in a marketing campaign by Whirlpool’s Brazilian advertising agency. As generative models continue to advance, distinguishing between authentic and fabricated content is becoming increasingly difficult, raising serious concerns about the potential consequences of AI-driven misinformation on individuals’ reputations, careers, and public trust.

The aim of this study is to identify patterns in how AI-generated posts are created, where they appear online, and what types of content they contain. By analyzing these trends, we seek to better equip individuals to navigate the increasingly complex and confusing information landscape shaped by social media and artificial intelligence.

To accomplish this goal, we laid out a list of 9 questions which this dataset will allow us to answer:

  1. Which social media platform is most prone to AI-generated content?
  
  2. How long are posts that are typically flagged as AI-generated compared to other model sigantures?
  
  3. Is misinformation more likely to be posted by humans or generative AI?
  
  4. Are misinformation posts more likely to come from AI-generated content compared to legitimate posts?
  
  5. Do misinformation posts show higher toxicity levels than legitimate posts?
  
  6. Which country contributes the most misinformation content, and how does that compare to legitimate content?
  
  7. How does engagement (likes, shares, interactions) differ between misinformation and legitimate posts?
  
  8. Are certain sentiment trends (negative, neutral, positive) more common in misinformation posts?
  
  9. Do verified users spread misinformation at a higher or lower rate than unverified users?

# 2. Data

### 2.1. Load Data and Libraries
```{r setup, echo=TRUE, results="hide", message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)

library(tidyverse)
library(lubridate)
library(janitor)
library(dplyr)
library(tidytext)

df <- read_csv("generative_ai_misinformation_dataset.csv")
```

```{r load}
# Show first few rows
head(df)
```
### 2.2 Data Cleaning

This data cleaning step converts spaces in column names to underscores, forces them to be lowercase, and removes any special characters
for better use in data manipulation. 

```{r}
df <- df %>% clean_names()
```


This data cleaning step removes any rows that might have impossible values do not belong within certain bounds outlined on the dataset's website. 

```{r}
df <- df %>%
  filter(
    between(sentiment_score, -1, 1),
    between(toxicity_score, 0, 1),
    between(detected_synthetic_score, 0, 1),
    between(embedding_sim_to_facts, 0, 1),
    readability_score >= 0
  )

```

This data cleaning step removes any rows that do not have a determination on whether the post was created by generative AI.
```{r}
df <- df %>%
  drop_na(is_misinformation)
```

This data cleaning step removes any categorical text inconsistencies, and converts important variables into factors for better data manipulation.

```{r}
df <- df %>%
  mutate(
    platform = str_to_title(platform),
    country = str_to_title(country),
    city = str_to_title(city),
    factcheck_verdict = str_to_lower(factcheck_verdict)
  )

df$date <- as.Date(df$date)
df$platform <- as.factor(df$platform)
df$is_misinformation <- factor(df$is_misinformation, 
                                levels = c(0, 1), 
                                labels = c("Legitimate", "Misinformation"))
```

### 2.3 Dataset Information

#### 2.3.1 Number of rows, columns, and null rows
```{r description}
# Basic information
cat("Number of rows:", nrow(df), "\n")
cat("Number of columns:", ncol(df), "\n")
cat("Missing values:", sum(is.na(df)), "\n")
```

#### 2.3.2 Variables

- **Post information:** platform, text content, timestamp, location
- **Content metrics:** text length, readability score, sentiment, toxicity
- **AI detection:** model signature (GPT-like, human, unknown), synthetic score
- **Author info:** follower count, verification status
- **Target variable:** is_misinformation (0 = legitimate, 1 = misinformation)

#### 2.3.3 Basic Data Summaries

How much misinformation is there?

```{r misinformation}
# Count misinformation vs legitimate
table(df$is_misinformation)

# Make a bar plot
ggplot(df, aes(x = is_misinformation, fill = is_misinformation)) +
  geom_bar() +
  geom_text(stat = 'count', aes(label = after_stat(count)), vjust = -0.5) +
  scale_fill_manual(values = c("Legitimate" = "green", "Misinformation" = "red")) +
  labs(title = "Misinformation vs Legitimate Posts", 
       x = "Type", y = "Count") +
  theme_minimal() +
  theme(legend.position = "none")
```

**Finding:** Out of 500 posts, 259 (51.8%) are misinformation and 241 (48.2%) are legitimate. This is a fairly balanced dataset, which is good for building predictive models.


How much content is AI-generated?

```{r model_signature}
# Count by model signature
table(df$model_signature)

# Bar plot
ggplot(df, aes(x = model_signature, fill = model_signature)) +
  geom_bar() +
  geom_text(stat = 'count', aes(label = after_stat(count)), vjust = -0.5) +
  labs(title = "AI Detection: Model Signatures", 
       x = "Model Signature", y = "Count") +
  theme_minimal() +
  theme(legend.position = "none")
```

**Finding:** About one-third of posts (157) are detected as "GPT-like" (AI-generated), one-third are human-written (174), and one-third are unknown (169). This suggests AI-generated content is a significant presence in social media.

Which countries are represented?

```{r countries}
# Count by country
table(df$country)

# Bar plot
ggplot(df, aes(x = country, fill = country)) +
  geom_bar() +
  geom_text(stat = 'count', aes(label = after_stat(count)), vjust = -0.5) +
  labs(title = "Posts by Country", x = "Country", y = "Count") +
  theme_minimal() +
  theme(legend.position = "none")
```

**Finding:** The dataset covers 5 countries with relatively equal representation (~86-113 posts each). This geographic diversity means our findings can potentially apply across different regions.

Which platforms have the most posts?

```{r platforms}
# Count by platform
table(df$platform)

# Bar plot
ggplot(df, aes(x = platform, fill = platform)) +
  geom_bar() +
  geom_text(stat = 'count', aes(label = after_stat(count)), vjust = -0.5) +
  labs(title = "Posts by Platform", x = "Platform", y = "Count") +
  theme_minimal() +
  theme(legend.position = "none")
```

**Finding:** Posts are fairly evenly distributed across all four platforms. Twitter has slightly more posts (129), while the others have around 121-26 posts each. This means our dataset isn't biased toward any single platform.

#### 2.3.4 Summary Statistics for Key Numbers

```{r summary_stats}
# Select important numerical variables
summary_vars <- df %>% 
  select(text_length, sentiment_score, toxicity_score, 
         detected_synthetic_score, engagement)

# Get summary statistics
summary(summary_vars)
```

**What these numbers mean:**

- **Text length:** Posts average 150 characters (range: 20-280). Most posts are relatively short.
- **Sentiment score:** Ranges from -1 (very negative) to 1 (very positive). Average is near 0, meaning posts are generally neutral.
- **Toxicity score:** Ranges from 0 (not toxic) to 1 (very toxic). Average is 0.49, suggesting moderate toxicity overall.
- **Synthetic score:** Higher scores mean more likely to be AI-generated. Wide range suggests varying confidence in detection.
- **Engagement:** Highly variable (4 to 9,977), meaning some posts go viral while others get little attention.

#### Data Quality Check
```{r quality_check}
cat("Data Quality Summary:\n")
cat("- Total records:", nrow(df), "\n")
cat("- Missing values:", sum(is.na(df)), "\n")
cat("- Duplicate post IDs:", sum(duplicated(df$post_id)), "\n")
```

**Quality notes:** The dataset is clean with no missing values or duplicates. All 500 records are complete and ready for use.

# 3. Findings

### 3.1. Which social media platform is most prone to AI-generated content?

```{r}
# Calculate AI-generated posts percentage by platform
ai_percentage <- df %>%
  group_by(platform) %>%
  summarise(
    total_posts = n(),
    ai_posts = sum(model_signature == "GPT-like")
  ) %>%
  mutate(ai_percentage = (ai_posts / total_posts) * 100)

# Create bar plot
ggplot(ai_percentage, aes(x = platform, y = ai_percentage, fill = platform)) +
  geom_col() +
  geom_text(aes(label = round(ai_percentage, 1)), vjust = -0.5) +
  scale_y_continuous(expand = expansion(mult = c(0, 0.1))) +
  labs(
    title = "Percentage of AI-Generated Posts by Platform",
    x = "Platform",
    y = "AI-Generated Posts (%)"
  ) +
  theme_minimal() +
  theme(legend.position = "none")
```

The findings from this plot seem to indicate that out of all the posts logged for all the platforms, Reddit seems to have the highest percentage (34.7%) of posts that were deemed to be GPT-like, or in other words, potentially generated by AI. Twitter seems to have the lowest (28.7%), which does seem to make sense, as from personal use it tends to do a better job of sensing what AI generated content looks like, and notifying its user of that fact, making it less likely for posters to post AI generated content because it is more likely to be flagged. 

### 3.2 How long are posts that are typically flagged as AI-generated compared to other model sigantures?
```{r}
ggplot(df, aes(x = model_signature, y = text_length, fill = model_signature)) +
  geom_boxplot() +
  labs(
    title = "Post Length by Content Source",
    x = "Content Source",
    y = "Post Length (Characters)"
  ) +
  theme_minimal()

```

According to this graph, it seems as if GPT-like (or generative AI) created posts tend to be somewhat shorter in length compared to posts
flagged as human or unknown, with GPT-like posts having a text length that hovers around 140 characters, whereas the post length for human or unknown
flagged posts tend to be around 160-170 characters. So although not a significant difference in the length of the post when grouped by content source,
there is still a difference nonetheless.


### 3.3 Is misinformation more likely to be posted by humans or generative AI?
```{r}
df |>
  filter(is_misinformation == "Misinformation") |>
  count(model_signature) |>
  mutate(
    percentage = round((n / sum(n)) * 100, 1),
    label = paste0(percentage, "%")
  ) |>
  ggplot(aes(x = "", y = n, fill = model_signature)) +
  geom_col(width = 1) +
  coord_polar(theta = "y") +
  geom_text(aes(label = label),
            position = position_stack(vjust = 0.5)) +
  labs(
    title = "Proportion of All Misinformation Posts by Content Source",
    fill = "Content Source"
  ) +
  theme_void()

```

According to this table, there is a good chance that there is an even split in terms of the percentage of misinformation posts that are posted by humans vs generative AI models. Unknown has the biggest proportion of posts being flagged as misinformation, so if we split that percentage in half, and give the halves to the GPT-like and human propertions, they would both hover around 50%, indicating it is just as likely for a post containing misinformation to be posted by a human vs generated by an AI model.