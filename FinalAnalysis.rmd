---
title: "Research Questions Analysis: Q5, Q7, Q9"
author: "Mahathi, Maya, Nayan"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, fig.width = 10, fig.height = 6)
```

# Introduction

This document analyzes three key research questions from our misinformation dataset:
-**Q4:** Are misinformation posts more likely to come from AI-generated content compared to legitimate posts?
- **Q5:** Do misinformation posts show higher toxicity levels than legitimate posts?
- **Q6:** Which country contributes the most misinformation content, and how does that compare to legitimate content?
- **Q7:** How does engagement differ between misinformation and legitimate posts?
- **Q8:**  Are certain sentiment trends (negative, neutral, positive) more common in misinformation posts?
- **Q9:** Do verified users spread misinformation at a higher or lower rate than unverified users?

---

# Load Libraries and Data

```{r load}
library(ggplot2)
library(dplyr)
library(knitr)

# Load the dataset
df <- read.csv('generative_ai_misinformation_dataset.csv')

# Convert is_misinformation to factor for better labeling
df$is_misinformation <- factor(df$is_misinformation, 
                                levels = c(0, 1), 
                                labels = c("Legitimate", "Misinformation"))

cat("Dataset loaded successfully!")
cat("\nTotal posts:", nrow(df))
cat("\nLegitimate:", sum(df$is_misinformation == "Legitimate"))
cat("\nMisinformation:", sum(df$is_misinformation == "Misinformation"))
```

---

# Research Question 5: Toxicity Levels

**Question:** Do misinformation posts show higher toxicity levels than legitimate posts?

## Summary Statistics

```{r q5_summary}
# Calculate summary statistics for toxicity by type
toxicity_summary <- df %>%
  group_by(is_misinformation) %>%
  summarise(
    Mean = mean(toxicity_score),
    Median = median(toxicity_score),
    SD = sd(toxicity_score),
    Min = min(toxicity_score),
    Max = max(toxicity_score),
    Count = n()
  )

kable(toxicity_summary, digits = 3, caption = "Toxicity Score Statistics by Post Type")
```

**Interpretation:** The table above shows the descriptive statistics for toxicity scores. Toxicity scores range from 0 (not toxic) to 1 (very toxic).

## Statistical Test

```{r q5_test}
# Perform t-test
t_test_toxicity <- t.test(toxicity_score ~ is_misinformation, data = df)

# Display results
cat("Independent Samples T-Test\n")
cat("==========================\n")
cat("T-statistic:", round(t_test_toxicity$statistic, 4), "\n")
cat("P-value:", format.pval(t_test_toxicity$p.value, digits = 4), "\n")
cat("95% Confidence Interval:", round(t_test_toxicity$conf.int[1], 4), 
    "to", round(t_test_toxicity$conf.int[2], 4), "\n")
cat("\nMean difference (Legitimate - Misinformation):", 
    round(diff(t_test_toxicity$estimate), 4), "\n")

if (t_test_toxicity$p.value < 0.05) {
  cat("\n✓ Result: There IS a statistically significant difference in toxicity (p < 0.05)\n")
} else {
  cat("\n✗ Result: There is NO statistically significant difference in toxicity (p >= 0.05)\n")
}
```

## Visualization

```{r q5_viz, fig.height=7}
# Create boxplot
p1 <- ggplot(df, aes(x = is_misinformation, y = toxicity_score, 
                     fill = is_misinformation)) +
  geom_boxplot(alpha = 0.7, outlier.shape = 1) +
  geom_jitter(alpha = 0.2, width = 0.2, size = 1) +
  scale_fill_manual(values = c("Legitimate" = "#4CAF50", 
                                "Misinformation" = "#F44336")) +
  labs(title = "Toxicity Levels: Misinformation vs Legitimate Posts",
       subtitle = paste0("Mean Toxicity: Legitimate = ", 
                        round(toxicity_summary$Mean[1], 3),
                        ", Misinformation = ", 
                        round(toxicity_summary$Mean[2], 3)),
       x = "Post Type",
       y = "Toxicity Score (0-1)") +
  theme_minimal(base_size = 12) +
  theme(legend.position = "none",
        plot.title = element_text(face = "bold", size = 16),
        plot.subtitle = element_text(size = 12))

print(p1)
```

**Key Findings:**

- Legitimate posts have a mean toxicity of **`r round(toxicity_summary$Mean[1], 3)`**
- Misinformation posts have a mean toxicity of **`r round(toxicity_summary$Mean[2], 3)`**
- The difference is **`r round(abs(toxicity_summary$Mean[2] - toxicity_summary$Mean[1]), 3)`**
- Statistical significance: **`r ifelse(t_test_toxicity$p.value < 0.05, "YES", "NO")`** (p = `r round(t_test_toxicity$p.value, 4)`)

---

# Research Question 7: Engagement Differences

**Question:** How does engagement (likes, shares, interactions) differ between misinformation and legitimate posts?

## Summary Statistics

```{r q7_summary}
# Calculate engagement statistics
engagement_summary <- df %>%
  group_by(is_misinformation) %>%
  summarise(
    Mean = mean(engagement),
    Median = median(engagement),
    SD = sd(engagement),
    Min = min(engagement),
    Max = max(engagement),
    Q1 = quantile(engagement, 0.25),
    Q3 = quantile(engagement, 0.75),
    Count = n()
  )

kable(engagement_summary, digits = 1, caption = "Engagement Statistics by Post Type", 
      format.args = list(big.mark = ","))
```

**Interpretation:** Engagement metrics show the total interactions (likes, shares, comments) for each post. Note the wide range, indicating some posts go viral while others receive minimal attention.

## Statistical Test

```{r q7_test}
# Perform Wilcoxon test (better for skewed distributions)
wilcox_test_engagement <- wilcox.test(engagement ~ is_misinformation, data = df)

# Also perform t-test for completeness
t_test_engagement <- t.test(engagement ~ is_misinformation, data = df)

cat("Wilcoxon Rank Sum Test (Non-parametric)\n")
cat("========================================\n")
cat("W-statistic:", wilcox_test_engagement$statistic, "\n")
cat("P-value:", format.pval(wilcox_test_engagement$p.value, digits = 4), "\n")

cat("\n\nIndependent Samples T-Test (Parametric)\n")
cat("========================================\n")
cat("T-statistic:", round(t_test_engagement$statistic, 4), "\n")
cat("P-value:", format.pval(t_test_engagement$p.value, digits = 4), "\n")

if (wilcox_test_engagement$p.value < 0.05) {
  cat("\n✓ Result: There IS a statistically significant difference in engagement (p < 0.05)\n")
} else {
  cat("\n✗ Result: There is NO statistically significant difference in engagement (p >= 0.05)\n")
}
```

## Visualizations

```{r q7_viz1, fig.height=7}
# Boxplot with jittered points
p2 <- ggplot(df, aes(x = is_misinformation, y = engagement, 
                     fill = is_misinformation)) +
  geom_boxplot(alpha = 0.7, outlier.shape = NA) +
  geom_jitter(alpha = 0.3, width = 0.2, size = 1.5) +
  scale_fill_manual(values = c("Legitimate" = "#4CAF50", 
                                "Misinformation" = "#F44336")) +
  scale_y_continuous(labels = scales::comma) +
  labs(title = "Engagement Distribution: Misinformation vs Legitimate Posts",
       subtitle = paste0("Median Engagement: Legitimate = ", 
                        scales::comma(round(engagement_summary$Median[1], 0)),
                        ", Misinformation = ", 
                        scales::comma(round(engagement_summary$Median[2], 0))),
       x = "Post Type",
       y = "Engagement (likes, shares, interactions)") +
  theme_minimal(base_size = 12) +
  theme(legend.position = "none",
        plot.title = element_text(face = "bold", size = 16),
        plot.subtitle = element_text(size = 12))

print(p2)
```

```{r q7_viz2, fig.height=6}
# Bar chart comparing means
engagement_means <- df %>%
  group_by(is_misinformation) %>%
  summarise(Mean_Engagement = mean(engagement))

p2b <- ggplot(engagement_means, aes(x = is_misinformation, y = Mean_Engagement, 
                                    fill = is_misinformation)) +
  geom_bar(stat = "identity", alpha = 0.8, width = 0.6) +
  geom_text(aes(label = scales::comma(round(Mean_Engagement, 0))), 
            vjust = -0.5, fontface = "bold", size = 5) +
  scale_fill_manual(values = c("Legitimate" = "#4CAF50", 
                                "Misinformation" = "#F44336")) +
  scale_y_continuous(labels = scales::comma, expand = expansion(mult = c(0, 0.1))) +
  labs(title = "Average Engagement by Post Type",
       x = "Post Type",
       y = "Average Engagement") +
  theme_minimal(base_size = 12) +
  theme(legend.position = "none",
        plot.title = element_text(face = "bold", size = 16))

print(p2b)
```

**Key Findings:**

- Legitimate posts have a median engagement of **`r scales::comma(round(engagement_summary$Median[1], 0))`**
- Misinformation posts have a median engagement of **`r scales::comma(round(engagement_summary$Median[2], 0))`**
- The difference is **`r scales::comma(round(abs(engagement_summary$Median[2] - engagement_summary$Median[1]), 0))`**
- Statistical significance: **`r ifelse(wilcox_test_engagement$p.value < 0.05, "YES", "NO")`** (p = `r round(wilcox_test_engagement$p.value, 4)`)

---

# Research Question 9: Verified Users and Misinformation

**Question:** Do verified users spread misinformation at a higher or lower rate than unverified users?

## Contingency Table

```{r q9_table}
# Create contingency table
verification_table <- table(df$author_verified, df$is_misinformation)
rownames(verification_table) <- c("Unverified (0)", "Verified (1)")

kable(verification_table, caption = "Posts by Verification Status and Type")
```

## Misinformation Rates

```{r q9_rates}
# Calculate proportions
verification_props <- df %>%
  group_by(author_verified, is_misinformation) %>%
  summarise(Count = n(), .groups = 'drop') %>%
  group_by(author_verified) %>%
  mutate(Total = sum(Count),
         Percentage = (Count / Total) * 100)

# Extract misinformation rates
misinfo_rates <- verification_props %>%
  filter(is_misinformation == "Misinformation") %>%
  mutate(Verified_Status = ifelse(author_verified == 1, "Verified", "Unverified")) %>%
  select(Verified_Status, Count, Total, Percentage)

kable(misinfo_rates, digits = 1, caption = "Misinformation Rate by Verification Status")
```

**Interpretation:** This table shows what percentage of posts from each user type (verified vs. unverified) are misinformation.

## Statistical Test

```{r q9_test}
# Chi-square test
chi_test <- chisq.test(verification_table)

cat("Chi-Square Test of Independence\n")
cat("================================\n")
cat("Chi-square statistic:", round(chi_test$statistic, 4), "\n")
cat("Degrees of freedom:", chi_test$parameter, "\n")
cat("P-value:", format.pval(chi_test$p.value, digits = 4), "\n")

# Calculate effect size (Cramér's V)
cramers_v <- sqrt(chi_test$statistic / (nrow(df) * (min(dim(verification_table)) - 1)))
cat("Effect size (Cramér's V):", round(cramers_v, 4), "\n")

if (chi_test$p.value < 0.05) {
  cat("\n✓ Result: There IS a statistically significant association between\n")
  cat("  verification status and misinformation spread (p < 0.05)\n")
} else {
  cat("\n✗ Result: There is NO statistically significant association between\n")
  cat("  verification status and misinformation spread (p >= 0.05)\n")
}
```

## Visualizations

```{r q9_viz1, fig.height=7}
# Prepare data for visualization
verification_viz <- df %>%
  mutate(Verified = factor(author_verified, 
                           levels = c(0, 1), 
                           labels = c("Unverified", "Verified"))) %>%
  group_by(Verified, is_misinformation) %>%
  summarise(Count = n(), .groups = 'drop') %>%
  group_by(Verified) %>%
  mutate(Percentage = (Count / sum(Count)) * 100)

# Percentage bar chart
p3 <- ggplot(verification_viz, aes(x = Verified, y = Percentage, 
                                   fill = is_misinformation)) +
  geom_bar(stat = "identity", position = "dodge", alpha = 0.8, width = 0.7) +
  geom_text(aes(label = paste0(round(Percentage, 1), "%")), 
            position = position_dodge(width = 0.7), 
            vjust = -0.5, fontface = "bold", size = 4) +
  scale_fill_manual(values = c("Legitimate" = "#4CAF50", 
                                "Misinformation" = "#F44336")) +
  labs(title = "Misinformation Spread by User Verification Status",
       subtitle = "Percentage of posts that are misinformation vs legitimate",
       x = "User Verification Status",
       y = "Percentage of Posts (%)",
       fill = "Post Type") +
  theme_minimal(base_size = 12) +
  theme(plot.title = element_text(face = "bold", size = 16),
        plot.subtitle = element_text(size = 12),
        legend.position = "bottom",
        legend.title = element_text(face = "bold"))

print(p3)
```

```{r q9_viz2, fig.height=6}
# Raw counts bar chart
p3b <- ggplot(verification_viz, aes(x = Verified, y = Count, 
                                    fill = is_misinformation)) +
  geom_bar(stat = "identity", position = "dodge", alpha = 0.8, width = 0.7) +
  geom_text(aes(label = Count), 
            position = position_dodge(width = 0.7), 
            vjust = -0.5, fontface = "bold", size = 4) +
  scale_fill_manual(values = c("Legitimate" = "#4CAF50", 
                                "Misinformation" = "#F44336")) +
  labs(title = "Raw Counts: Posts by Verification Status and Type",
       x = "User Verification Status",
       y = "Number of Posts",
       fill = "Post Type") +
  theme_minimal(base_size = 12) +
  theme(plot.title = element_text(face = "bold", size = 16),
        legend.position = "bottom",
        legend.title = element_text(face = "bold"))

print(p3b)
```

**Key Findings:**

```{r q9_findings, echo=FALSE}
verified_misinfo_rate <- misinfo_rates$Percentage[misinfo_rates$Verified_Status == "Verified"]
unverified_misinfo_rate <- misinfo_rates$Percentage[misinfo_rates$Verified_Status == "Unverified"]
```

- **Verified users:** `r round(verified_misinfo_rate, 1)`% of their posts are misinformation
- **Unverified users:** `r round(unverified_misinfo_rate, 1)`% of their posts are misinformation
- **Difference:** `r round(abs(verified_misinfo_rate - unverified_misinfo_rate), 1)` percentage points
- **Statistical significance:** `r ifelse(chi_test$p.value < 0.05, "YES", "NO")` (p = `r round(chi_test$p.value, 4)`)

---

# Summary of All Findings

```{r summary_table, echo=FALSE}
# Create summary table
summary_df <- data.frame(
  Question = c("Q5: Toxicity", "Q7: Engagement", "Q9: Verification"),
  Finding = c(
    paste0("Misinformation has ", 
           ifelse(toxicity_summary$Mean[2] > toxicity_summary$Mean[1], "higher", "lower"),
           " toxicity (", round(toxicity_summary$Mean[2], 3), " vs ", 
           round(toxicity_summary$Mean[1], 3), ")"),
    paste0("Misinformation has ", 
           ifelse(engagement_summary$Median[2] > engagement_summary$Median[1], "higher", "lower"),
           " engagement (median: ", scales::comma(round(engagement_summary$Median[2], 0)), 
           " vs ", scales::comma(round(engagement_summary$Median[1], 0)), ")"),
    paste0("Verified users: ", round(verified_misinfo_rate, 1), 
           "% misinfo, Unverified: ", round(unverified_misinfo_rate, 1), "% misinfo")
  ),
  Statistically_Significant = c(
    ifelse(t_test_toxicity$p.value < 0.05, "Yes", "No"),
    ifelse(wilcox_test_engagement$p.value < 0.05, "Yes", "No"),
    ifelse(chi_test$p.value < 0.05, "Yes", "No")
  ),
  P_Value = c(
    round(t_test_toxicity$p.value, 4),
    round(wilcox_test_engagement$p.value, 4),
    round(chi_test$p.value, 4)
  )
)

kable(summary_df, caption = "Summary of Research Questions 5, 7, and 9")
```

## Research Question 4: AI-Generated Content and Misinformation

**Question:** Are misinformation posts more likely to come from AI-generated content compared to legitimate posts?

```{r simplier}
# Create a simpler AI vs non-AI flag

df <- df %>%
mutate(ai_generated = ifelse(model_signature == "GPT-like",
"AI-generated",
"Non-AI / Unknown"))

df$ai_generated <- factor(df$ai_generated,
levels = c("AI-generated", "Non-AI / Unknown"))
```


```{r tableAI}

# Contingency table of AI status by misinformation

ai_table <- table(df$ai_generated, df$is_misinformation)

kable(ai_table,
caption = "Posts by AI-generation status and misinformation label")

```

```{r misinformation}
# Misinformation vs legitimate percentages within each AI group

ai_props <- df %>%
group_by(ai_generated, is_misinformation) %>%
summarise(Count = n(), .groups = "drop") %>%
group_by(ai_generated) %>%
mutate(Total = sum(Count),
Percentage = 100 * Count / Total)

kable(ai_props,
digits = 1,
caption = "Percentage of misinformation vs legitimate posts by AI status")

```

```{r chi square}
# Chi-square test of independence

chi_ai <- chisq.test(ai_table)

cat("Chi-Square Test of Independence (AI-generated x Misinformation)\n")
cat("================================================================\n")
cat("Chi-square statistic:", round(chi_ai$statistic, 4), "\n")
cat("Degrees of freedom:", chi_ai$parameter, "\n")
cat("P-value:", format.pval(chi_ai$p.value, digits = 4), "\n")
```


```{r barchart-percentage}
# Bar chart of percentages

ai_viz <- ai_props

ggplot(ai_viz,
aes(x = ai_generated,
y = Percentage,
fill = is_misinformation)) +
geom_col(position = "dodge", alpha = 0.85, width = 0.7) +
geom_text(aes(label = paste0(round(Percentage, 1), "%")),
position = position_dodge(width = 0.7),
vjust = -0.4,
size = 4,
fontface = "bold") +
labs(title = "Share of Misinformation vs Legitimate Posts",
subtitle = "Broken down by AI-generated vs Non-AI/Unknown content",
x = "Content type",
y = "Percentage of posts",
fill = "Post type") +
theme_minimal(base_size = 12) +
theme(plot.title = element_text(face = "bold", size = 16),
legend.position = "bottom")
```

```{r pull rates}
# Pull out just the misinformation rates

ai_misinfo <- ai_props %>%
filter(is_misinformation == "Misinformation")

ai_rate_ai  <- ai_misinfo$Percentage[ai_misinfo$ai_generated == "AI-generated"]
ai_rate_non <- ai_misinfo$Percentage[ai_misinfo$ai_generated == "Non-AI / Unknown"]


```
## Research Question 6: 

**Question:** Which country contributes the most misinformation content, and how does that compare to legitimate content?


```{r country_analysis}
country_summary <- df %>%
group_by(country, is_misinformation) %>%
summarise(Count = n(), .groups = "drop") %>%
group_by(country) %>%
mutate(Total = sum(Count),
Percentage = 100 * Count / Total)

kable(country_summary,
digits = 1,
caption = "Misinformation vs Legitimate Posts by Country")
```

```{r most misinfomration}
country_misinfo <- country_summary %>%
filter(is_misinformation == "Misinformation") %>%
arrange(desc(Count))

kable(country_misinfo,
digits = 1,
caption = "Countries Ranked by Misinformation Post Count")
```


```{r stat_test}
country_table <- table(df$country, df$is_misinformation)

chi_country <- chisq.test(country_table)

cat("Chi-Square Test of Independence (Country x Misinformation)\n")
cat("=========================================================\n")
cat("Chi-square statistic:", round(chi_country$statistic, 4), "\n")
cat("Degrees of freedom:", chi_country$parameter, "\n")
cat("P-value:", format.pval(chi_country$p.value, digits = 4), "\n")
```

```{r percentage by country}
misinfo_share_country <- country_summary %>%
filter(is_misinformation == "Misinformation")

ggplot(misinfo_share_country,
aes(x = reorder(country, -Percentage),
y = Percentage)) +
geom_col(alpha = 0.85, width = 0.7, fill = "#F44336") +
geom_text(aes(label = paste0(round(Percentage, 1), "%")),
vjust = -0.4,
size = 4,
fontface = "bold") +
labs(title = "Share of Misinformation Posts by Country",
x = "Country",
y = "Misinformation as % of Country's Total Posts") +
theme_minimal(base_size = 12) +
theme(plot.title = element_text(face = "bold", size = 16))
```

```{r keyf}
top_country <- misinfo_share_country$country[which.max(misinfo_share_country$Count)]
top_country_pct <- misinfo_share_country$Percentage[which.max(misinfo_share_country$Count)]
```
## Research Question 8: 

**Question:**  Are certain sentiment trends (negative, neutral, positive) more common in misinformation posts?

```{r sentiment cate.}
library(dplyr)

df <- df %>%
mutate(sentiment_category = case_when(
sentiment_score <= -0.2 ~ "Negative",
sentiment_score >=  0.2 ~ "Positive",
TRUE                    ~ "Neutral"
))

df$sentiment_category <- factor(df$sentiment_category,
levels = c("Negative", "Neutral", "Positive"))
```


```{r ctable_percentage}
library(knitr)

sentiment_summary <- df %>%
group_by(sentiment_category, is_misinformation) %>%
summarise(Count = n(), .groups = "drop") %>%
group_by(sentiment_category) %>%
mutate(Total = sum(Count),
Percentage = 100 * Count / Total)

kable(sentiment_summary,
digits = 1,
caption = "Misinformation vs Legitimate Posts by Sentiment Category")
```



```{r sentiment_mis}
sentiment_table <- table(df$sentiment_category, df$is_misinformation)

chi_sentiment <- chisq.test(sentiment_table)

cat("Chi-Square Test of Independence (Sentiment x Misinformation)\n")
cat("===========================================================\n")
cat("Chi-square statistic:", round(chi_sentiment$statistic, 4), "\n")
cat("Degrees of freedom:", chi_sentiment$parameter, "\n")
cat("P-value:", format.pval(chi_sentiment$p.value, digits = 4), "\n")
```


```{r pltx}
library(ggplot2)

ggplot(sentiment_summary,
aes(x = sentiment_category,
y = Percentage,
fill = is_misinformation)) +
geom_col(position = "dodge", alpha = 0.85, width = 0.7) +
geom_text(aes(label = paste0(round(Percentage, 1), "%")),
position = position_dodge(width = 0.7),
vjust = -0.4,
size = 4,
fontface = "bold") +
labs(title = "Misinformation vs Legitimate Posts by Sentiment Category",
x = "Sentiment Category",
y = "Percentage within Each Sentiment Group",
fill = "Post Type") +
theme_minimal(base_size = 12) +
theme(plot.title = element_text(face = "bold", size = 16),
legend.position = "bottom")
```


```{r finalkeyfinds}
sent_misinfo <- sentiment_summary %>%
filter(is_misinformation == "Misinformation") %>%
arrange(desc(Percentage))

top_sent <- sent_misinfo$sentiment_category[1]
top_sent_pct <- sent_misinfo$Percentage[1]
```
## Implications 

Based on our analysis:

1. **Toxicity (Q5):** `r ifelse(t_test_toxicity$p.value < 0.05, "Toxicity is significantly different between misinformation and legitimate posts, suggesting it could be a useful feature for detection models.", "Toxicity alone may not be a strong discriminator between misinformation and legitimate content.")`

2. **Engagement (Q7):** `r ifelse(wilcox_test_engagement$p.value < 0.05, "Engagement patterns differ significantly, indicating that misinformation may spread differently than legitimate content on social media.", "Engagement levels are similar regardless of content veracity, suggesting misinformation spreads as widely as legitimate information.")`

3. **Verification (Q9):** `r ifelse(chi_test$p.value < 0.05, "User verification status is significantly associated with misinformation spread, which has important implications for platform trust systems.", "Verification status does not significantly predict misinformation spread, suggesting verified badges alone may not prevent misinformation.")`

---

# Conclusion

This analysis provides insights into the characteristics of misinformation on social media platforms. Understanding these patterns can help in developing better detection systems and informing platform policies.

**Data Citation:** Generative AI Misinformation Detection Dataset (n = 500 posts)

**Analysis Date:** `r Sys.Date()`